{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: rank_bm25 in /srv/conda/envs/notebook/lib/python3.7/site-packages (0.2)\n",
      "Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (from rank_bm25) (1.18.2)\n",
      "Requirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (1.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.7/site-packages (1.18.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: html2text in /srv/conda/envs/notebook/lib/python3.7/site-packages (2020.1.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 nltk\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/messages.csv', header=None)\n",
    "data.columns= ['subject','sender','to','date','empty','body']\n",
    "data=data.drop(columns=['empty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"to\"]==\"s.w.wintels@gmail.com\"]\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words if word.isalnum() \n",
    "                                             and not word in english_stopwords\n",
    "                                             and not (word.isnumeric() and len(word) < 4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = False\n",
    "#h.protect_links = True\n",
    "#h.single_line_breaks = True\n",
    "h.body_width=0\n",
    "\n",
    "def t2h (string):\n",
    "    return h.handle(string).replace(\"\\n\",\" \")\n",
    "\n",
    "def preprocess(string):\n",
    "    string = t2h(string)\n",
    "    return tokenize(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_words(needles, haystack):\n",
    "    if \"subject:\" in haystack:\n",
    "        return False\n",
    "    for needle in needles:\n",
    "        if needle in haystack:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocess(body) for body in data[\"body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query,n=3):\n",
    "    \n",
    "    if len(query)<4:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    tokenized_query = query.lower().split(\" \")\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    top_x_results = np.argsort(doc_scores)[::-1][:n]\n",
    "    top_x_scores = np.sort(doc_scores)[::-1][:n]\n",
    "    \n",
    "    results = []\n",
    "    for j in range(len(top_x_results)):\n",
    "        x = top_x_results[j]\n",
    "        sentences = t2h(data.loc[x]['body']).split('---')\n",
    "        good_sentences = [e for e in sentences if contains_words(tokenized_query, e.lower())]\n",
    "        for sentence in good_sentences:\n",
    "            sub_sentences = sentence.split('. ')\n",
    "            good_sub_sentences = [e for e in sub_sentences if contains_words(tokenized_query, e.lower())]\n",
    "            \n",
    "            links=re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sentence)\n",
    "            for i in range(len(links)):\n",
    "                    sentence = sentence.replace(links[i],f\"<a target=_blank href='{links[i][:-1]}'>Link {i+1}</a>)\")\n",
    "            for sub_sentence in good_sub_sentences:\n",
    "                sub_sentence = sub_sentence.strip().replace(\"|\",\"\")\n",
    "                links=re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sub_sentence)\n",
    "                for i in range(len(links)):\n",
    "                    sub_sentence = sub_sentence.replace(links[i],f\"<a target=_blank href='{links[i][:-1]}'>Link {i+1}</a>)\")\n",
    "                    #links[i]=\n",
    "                record = pd.Series([x,data[\"date\"][x],top_x_scores[j],sub_sentence])#, sentence])\n",
    "                results.append(record)\n",
    "            \n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = ['id','date','score','sub_sentence']#,'paragraph']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4e485278e84a53b156acfea7df4bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='Walmart', description='SearchTerms'), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "def search_papers(SearchTerms: str):\n",
    "    search_results = search(SearchTerms,20)\n",
    "    if len(search_results) > 0:\n",
    "        display(HTML(search_results.to_html(escape=False)))\n",
    "        #display(search_results) \n",
    "    return search_results\n",
    "\n",
    "searchbar = widgets.interactive(search_papers, SearchTerms='Walmart')\n",
    "searchbar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
